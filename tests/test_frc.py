#!/usr/bin/env python3
"""
FRCæ¨¡å‹æ¨ç†æµ‹è¯•ç¨‹åº
æµ‹è¯•Llama-3.1-8B-Instructæ¨¡å‹çš„åŸºæœ¬æ¨ç†åŠŸèƒ½
"""

import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def test_model_inference():
    """æµ‹è¯•æ¨¡å‹æ¨ç†åŠŸèƒ½"""
    
    # æ¨¡å‹æ–‡ä»¶è·¯å¾„
    model_path = project_root / "services" / "models" / "Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf"
    
    print(f"ğŸ” æ£€æŸ¥æ¨¡å‹æ–‡ä»¶: {model_path}")
    
    if not model_path.exists():
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return False
    
    print(f"âœ… æ¨¡å‹æ–‡ä»¶å­˜åœ¨ï¼Œå¤§å°: {model_path.stat().st_size / (1024**3):.2f} GB")
    
    try:
        # å¯¼å…¥llama-cpp-python
        print("ğŸ“¦ å¯¼å…¥llama-cpp-python...")
        from llama_cpp import Llama
        print("âœ… llama-cpp-pythonå¯¼å…¥æˆåŠŸ")
        
        # åˆå§‹åŒ–æ¨¡å‹
        print("ğŸš€ åˆå§‹åŒ–Llamaæ¨¡å‹...")
        print("âš ï¸  æ³¨æ„: æ¨¡å‹åŠ è½½å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´...")
        
        llm = Llama(
            model_path=str(model_path),
            n_gpu_layers=35,  # æ ¹æ®æ–‡æ¡£å»ºè®®
            n_ctx=2048,       # å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦ä»¥èŠ‚çœå†…å­˜
            verbose=True,     # æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
            n_threads=4       # çº¿ç¨‹æ•°
        )
        
        print("âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ!")
        
        # è¿›è¡Œç®€å•æ¨ç†æµ‹è¯•
        print("\nğŸ¤– å¼€å§‹æ¨ç†æµ‹è¯•...")
        
        test_prompt = "å˜¿æåç°åœ¨å‡ ç‚¹é’Ÿ"
        print(f"ğŸ“ è¾“å…¥æç¤º: {test_prompt}")
        print("ğŸ’­ ç”Ÿæˆå›å¤ä¸­...")
        
        response = llm(
            test_prompt,
            max_tokens=100,
            temperature=0.7,
            echo=False,
            stop=["Human:", "Assistant:"]
        )
        
        generated_text = response['choices'][0]['text'].strip()
        
        print(f"\nğŸ¯ æ¨¡å‹å›å¤: {generated_text}")
        print(f"\nğŸ“Š æ¨ç†ç»Ÿè®¡:")
        print(f"   - ç”Ÿæˆtokenæ•°: {response['usage']['completion_tokens']}")
        print(f"   - è¾“å…¥tokenæ•°: {response['usage']['prompt_tokens']}")
        print(f"   - æ€»tokenæ•°: {response['usage']['total_tokens']}")
        
        print("\nâœ… æ¨¡å‹æ¨ç†æµ‹è¯•æˆåŠŸ!")
        return True
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
        print("ğŸ’¡ è¯·å…ˆå®‰è£…llama-cpp-python:")
        print("   pip install llama-cpp-python")
        print("   æˆ–è€…å¦‚æœéœ€è¦CUDAæ”¯æŒ:")
        print("   CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python")
        return False
        
    except Exception as e:
        print(f"âŒ æ¨ç†æµ‹è¯•å¤±è´¥: {e}")
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        return False

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ¯ FRCæ¨¡å‹æ¨ç†æµ‹è¯•ç¨‹åº")
    print("=" * 60)
    
    success = test_model_inference()
    
    print("\n" + "=" * 60)
    if success:
        print("ğŸ‰ æµ‹è¯•å®Œæˆ - æ¨¡å‹å·¥ä½œæ­£å¸¸!")
    else:
        print("ğŸ’¥ æµ‹è¯•å¤±è´¥ - è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
    print("=" * 60)
